{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于正负样本不均衡，故提出以下方案\n",
    "\n",
    "**解决方案**\n",
    "1. 采用分治集成模型。将负类数据通过高斯混合聚类算法划分为15个块，使用负类中的一个块与正类数据训练模型，可获得15个模型。取预测的概率值的均值，作为最终的判断\n",
    "\n",
    "2. 以auc_roc指标作为优化目标训练模型\n",
    "\n",
    "3. 训练一个神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "matrix = pd.read_csv('./result/matrix.csv')\n",
    "matrix = matrix.drop(matrix.columns[0],axis=1)\n",
    "\n",
    "#train、test-setdata\n",
    "train_data = matrix[matrix['origin'] == 'train'].drop(['origin'], axis=1)\n",
    "test_data = matrix[matrix['origin'] == 'test'].drop(['label', 'origin'], axis=1)\n",
    "train_X, train_y = train_data.drop(['label'], axis=1), train_data['label']\n",
    "\n",
    "#导入分析库\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=.3)\n",
    "# 最终版提交，使用所有训练数据，训练模型\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.1)\n",
    "X_train, X_valid, y_train, y_valid = X_train.values, X_valid.values, y_train.values.astype('float32'), y_valid.values.astype('float32') \n",
    "test_data = test_data.values\n",
    "\n",
    "gc.collect()\n",
    "# del data_train,temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案一: 聚合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xgboost as xgb\n",
    "from model import model_train\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "model_dict = {'svm':SVC,'logistic':LogisticRegression,'decision_tree':DecisionTreeClassifier,'bayes':GaussianNB,\n",
    "               'mixture':GaussianMixture,'KNN':KNeighborsClassifier,'GBDT':GradientBoostingClassifier,'XGBoost':XGBClassifier}\n",
    "\n",
    "std = StandardScaler()\n",
    "std = std.fit(X_train)\n",
    "clf = model_dict['mixture'](n_components=15)\n",
    "clf = clf.fit(std.transform(X_train))\n",
    "x_train_cat = clf.predict(X_train)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.countplot(x_train_cat)\n",
    "plt.show()\n",
    "\n",
    "# 结果不理想\n",
    "indices_11 = (x_train_cat==11)\n",
    "X_train_11 = X_train[indices_11]\n",
    "y_train_11 = y_train[indices_11]\n",
    "model_train('logistic',model=model_dict['logistic'],x_train=X_train_11,y_train=y_train_11,x_val=X_valid,y_val=y_valid)\n",
    "model_train('bayes',model=model_dict['bayes'],x_train=X_train_11,y_train=y_train_11,x_val=X_valid,y_val=y_valid)\n",
    "model_train('decision_tree',model=model_dict['decision_tree'],x_train=X_train_11,y_train=y_train_11,x_val=X_valid,y_val=y_valid)\n",
    "model_train('XGBoost',model=model_dict['XGBoost'],x_train=X_train_11,y_train=y_train_11,x_val=X_valid,y_val=y_valid)\n",
    "\n",
    "del X_train_11,y_train_11,x_train_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方案一更正：\n",
    "使用高斯混合大多数的类别都聚集在第12个类别上，样本本质上还是不平衡。故采取随机选择的策略获取15个块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_indices = (y_train==0)\n",
    "true_indices = (y_train==1)\n",
    "\n",
    "x_train_true = X_train[true_indices]\n",
    "y_train_true = y_train[true_indices]\n",
    "\n",
    "x_train_false = X_train[false_indices]\n",
    "y_train_false = y_train[false_indices]\n",
    "\n",
    "true_count = y_train_true.shape[0]\n",
    "x_train_false_list = []\n",
    "y_train_false_list = []\n",
    "for i in range(15):\n",
    "    if i < 14:\n",
    "        x_train_false_list.append(x_train_false[true_count*i:(i+1)*true_count])\n",
    "        y_train_false_list.append(y_train_false[true_count*i:(i+1)*true_count])\n",
    "    else:\n",
    "        x_train_false_list.append(x_train_false[true_count*i:])\n",
    "        y_train_false_list.append(y_train_false[true_count*i:])\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "for i in range(len(x_train_false_list)):\n",
    "    x_train_i = x_train_false_list[i]\n",
    "    y_train_i = y_train_false_list[i]\n",
    "    # 将正负类进行连接，并打乱\n",
    "    x_train_son = np.concatenate((x_train_i,x_train_true),axis=0)\n",
    "    y_train_son = np.concatenate((y_train_i,y_train_true),axis=0)\n",
    "    shuffler = np.random.permutation(len(x_train_son))\n",
    "    x_train_son = x_train_son[shuffler]\n",
    "    y_train_son = y_train_son[shuffler]\n",
    "    x_train_list.append(x_train_son)\n",
    "    y_train_list.append(y_train_son)\n",
    "\n",
    "del x_train_true,x_train_false,y_train_true,y_train_false,x_train_false_list,y_train_false_list\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**集成模型**\n",
    "\n",
    "1. 聚合模型：对这15个模型的预测概率值，做均值处理\n",
    "\n",
    "2. 将聚合模型用于验证集的测试\n",
    "\n",
    "3. 生成submission,观察效果。 分数：使用GBDT,score:0.6654492; 使用XGB,score:0.6701968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:00:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第1个模型选择了XGB,其AUC值为0.6959346769569423\n",
      "[18:00:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第2个模型选择了XGB,其AUC值为0.6957330795639762\n",
      "[18:00:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第3个模型选择了XGB,其AUC值为0.6993525368216875\n",
      "[18:00:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第4个模型选择了XGB,其AUC值为0.6962452597750951\n",
      "[18:01:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第5个模型选择了XGB,其AUC值为0.6968651047644838\n",
      "[18:01:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第6个模型选择了XGB,其AUC值为0.6956352491176028\n",
      "[18:01:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第7个模型选择了XGB,其AUC值为0.697829510381054\n",
      "[18:01:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第8个模型选择了XGB,其AUC值为0.7018639969596914\n",
      "[18:01:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第9个模型选择了XGB,其AUC值为0.6996558881363338\n",
      "[18:01:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第10个模型选择了XGB,其AUC值为0.7018222619527521\n",
      "[18:01:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第11个模型选择了XGB,其AUC值为0.6886301788591866\n",
      "[18:01:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第12个模型选择了XGB,其AUC值为0.6971585448163454\n",
      "[18:01:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第13个模型选择了XGB,其AUC值为0.6990117201644064\n",
      "[18:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第14个模型选择了XGB,其AUC值为0.698276619882429\n",
      "[18:02:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第15个模型选择了XGB,其AUC值为0.7023655607082154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_agg = []\n",
    "roc_list = []\n",
    "\n",
    "def model_train_i(clf,x_train_i,y_train_i,X_valid,y_valid):\n",
    "    clf = clf.fit(x_train_i,y_train_i)\n",
    "    y_val_pred = clf.predict_proba(X_valid)\n",
    "    roc = roc_auc_score(y_true=y_valid, y_score=y_val_pred[:,1])    \n",
    "    return clf,roc\n",
    "\n",
    "for i in range(len(x_train_list)):\n",
    "    x_train_i = x_train_list[i]\n",
    "    y_train_i = y_train_list[i]\n",
    "    XGB = model_dict['XGBoost'](\n",
    "    # max_depth=8,\n",
    "    max_depth=6,\n",
    "    n_estimators=300,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.3,    \n",
    "    seed=42,\n",
    "    n_jobs=-1)\n",
    "    XGB, roc_X = model_train_i(XGB,x_train_i,y_train_i,X_valid,y_valid)\n",
    "    model_agg.append({'XGB':XGB})\n",
    "    print (f'第{i+1}个模型选择了XGB,其AUC值为{roc_X}')\n",
    "    # if roc_G > roc_X:\n",
    "    #     model_agg.append({'GBDT':GBDT})\n",
    "    #     print (f'第{i+1}个模型选择了GBDT,其AUC值为{roc_G}')\n",
    "    # else:\n",
    "    #     model_agg.append({'XGB':XGB})\n",
    "    #     print (f'第{i+1}个模型选择了XGB,其AUC值为{roc_X}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:02:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第1个模型选择了XGB,其AUC值为0.7059993912202367\n",
      "[18:02:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第2个模型选择了XGB,其AUC值为0.7107033303894447\n",
      "[18:02:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第3个模型选择了XGB,其AUC值为0.7095904353365111\n",
      "[18:02:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第4个模型选择了XGB,其AUC值为0.7065937977292054\n",
      "[18:02:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第5个模型选择了XGB,其AUC值为0.7072624040800041\n",
      "[18:02:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第6个模型选择了XGB,其AUC值为0.7073266029063465\n",
      "[18:02:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第7个模型选择了XGB,其AUC值为0.7076047465332377\n",
      "[18:02:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第8个模型选择了XGB,其AUC值为0.7103902601392347\n",
      "[18:02:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第9个模型选择了XGB,其AUC值为0.7087499140938414\n",
      "[18:02:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第10个模型选择了XGB,其AUC值为0.7070054677346727\n",
      "[18:02:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第11个模型选择了XGB,其AUC值为0.7054359109283918\n",
      "[18:02:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第12个模型选择了XGB,其AUC值为0.7068408997426405\n",
      "[18:02:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第13个模型选择了XGB,其AUC值为0.707044176793336\n",
      "[18:02:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第14个模型选择了XGB,其AUC值为0.7063917131072797\n",
      "[18:02:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "第15个模型选择了XGB,其AUC值为0.7138904873417916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score ,roc_auc_score\n",
    "\n",
    "model_agg = []\n",
    "roc_list = []\n",
    "\n",
    "def model_train_i(clf,x_train_i,y_train_i,X_valid,y_valid):\n",
    "    clf = clf.fit(x_train_i,y_train_i)\n",
    "    y_val_pred = clf.predict_proba(X_valid)\n",
    "    roc = roc_auc_score(y_true=y_valid, y_score=y_val_pred[:,1])    \n",
    "    return clf,roc\n",
    "\n",
    "for i in range(len(x_train_list)):\n",
    "    x_train_i = x_train_list[i]\n",
    "    y_train_i = y_train_list[i]\n",
    "    XGB = model_dict['XGBoost'](\n",
    "    # max_depth=8,\n",
    "    max_depth=8,\n",
    "    n_estimators=100,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.3,    \n",
    "    seed=42,\n",
    "    n_jobs=-1)\n",
    "    XGB, roc_X = model_train_i(XGB,x_train_i,y_train_i,X_valid,y_valid)\n",
    "    model_agg.append({'XGB':XGB})\n",
    "    print (f'第{i+1}个模型选择了XGB,其AUC值为{roc_X}')\n",
    "    # if roc_G > roc_X:\n",
    "    #     model_agg.append({'GBDT':GBDT})\n",
    "    #     print (f'第{i+1}个模型选择了GBDT,其AUC值为{roc_G}')\n",
    "    # else:\n",
    "    #     model_agg.append({'XGB':XGB})\n",
    "    #     print (f'第{i+1}个模型选择了XGB,其AUC值为{roc_X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**聚合模型总结**\n",
    "1. 由于样本不平衡，故使用此聚合模型可提升AUC值\n",
    "\n",
    "2. 试验后发现，该聚合模型能在baseline的基础上提升2-4个百分点。后期的迭代优化中，应考虑与其他方案进行融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集上表现出AUC:0.7168796626529258\n"
     ]
    }
   ],
   "source": [
    "def agg_model(model_agg,X_valid):\n",
    "    val_pred = None\n",
    "    for i in range(len(model_agg)):\n",
    "        clf = list(model_agg[i].values())[0]\n",
    "        y_valid_pred = np.expand_dims(clf.predict_proba(X_valid)[:,1],axis=0)\n",
    "        if i == 0:\n",
    "            val_pred = y_valid_pred\n",
    "        else:\n",
    "            val_pred = np.concatenate((val_pred,y_valid_pred),axis=0)\n",
    "    val_pred = val_pred.mean(axis=0)\n",
    "    return val_pred\n",
    "\n",
    "val_pred = agg_model(model_agg,X_valid)\n",
    "print (f'验证集上表现出AUC:{roc_auc_score(y_valid,val_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = './data_format1'\n",
    "submission = pd.read_csv(f'{paths}/test_format1.csv')\n",
    "\n",
    "prob = agg_model(model_agg,test_data)\n",
    "submission['prob'] = prob\n",
    "# submission.dorp(['origin'], axis=1, inplace=True)\n",
    "submission.to_csv('./result/submission_agg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案二：神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据转化为tensor,并归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "std = std.fit(X_train)\n",
    "X_train, X_valid = std.transform(X_train), std.transform(X_valid)\n",
    "test_data = std.transform(test_data)\n",
    "# 将原始数据转化为tensor类型\n",
    "device = torch.device('cuda')\n",
    "X_train_t = torch.Tensor(X_train).to(device)\n",
    "y_train_t = torch.Tensor(y_train).long().to(device)\n",
    "X_valid_t = torch.Tensor(X_valid).to(device)\n",
    "y_valid_t = torch.Tensor(y_valid).long().to(device)\n",
    "X_test_t = torch.Tensor(test_data).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "device = torch.device('cuda')\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim_list,out_dim=2):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc_list = []\n",
    "        for i in range(len(hidden_dim_list)):\n",
    "            hidden_dim = hidden_dim_list[i]\n",
    "            if i == 0:\n",
    "                self.fc_list.append(nn.Linear(in_dim,hidden_dim))\n",
    "                self.fc_list.append(nn.BatchNorm1d(hidden_dim))\n",
    "                self.fc_list.append(nn.Dropout())\n",
    "                self.fc_list.append(nn.ReLU())\n",
    "            else:\n",
    "                self.fc_list.append(nn.Linear(hidden_dim_list[i-1],hidden_dim))\n",
    "                self.fc_list.append(nn.BatchNorm1d(hidden_dim))\n",
    "                self.fc_list.append(nn.Dropout())\n",
    "                self.fc_list.append(nn.ReLU())\n",
    "        self.fc_list.append(nn.Linear(hidden_dim_list[-1],out_dim))\n",
    "        self.layers = nn.Sequential(*self.fc_list)\n",
    "\n",
    "    def forward(self,x):    \n",
    "        out = self.layers(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "in_dim = X_train_t.shape[1]\n",
    "hidden_dim_list = [68]\n",
    "out_dim = 2\n",
    "\n",
    "net = Net(in_dim,hidden_dim_list,out_dim).to(device)\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_train(net,X_train,y_train,batch_size=32,epoch=1,verbose=1):\n",
    "    net.train()\n",
    "    batch = int(X_train.shape[0]/batch_size)\n",
    "    for i in range(epoch):\n",
    "        for j in range(batch):\n",
    "            optimizer.zero_grad()\n",
    "            X_train_1 = X_train[j*batch_size:(j+1)*batch_size,:]\n",
    "            y_train_1 = y_train[j*batch_size:(j+1)*batch_size]\n",
    "            pred = net(X_train_1)\n",
    "            loss = loss_func(pred,y_train_1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if j % 1000 == 0 and verbose == 1:\n",
    "                print (f'第{j}个batch, 损失值为{loss}')\n",
    "        if verbose == 1:\n",
    "            print (f'\\nepoch {i+1}:loss equals to {loss}')\n",
    "# net_train(net,X_train_t,y_train_t,batch_size=120,epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络在验证的时候，全部的类都显示为父类，模型没有效果。\n",
    "\n",
    "**原因**：\n",
    "1. 神经网络对正负样本的比例要求严格。该数据集验证不平衡，故训练效果非常差\n",
    "\n",
    "**解决**\n",
    "1. 将方案一的方法与神经网络进行融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 估计网络没有学习到\n",
    "def net_eval(net,X_valid,y_valid):\n",
    "    net.eval()\n",
    "    pred = net(X_valid)\n",
    "    # torch.argmax(pred)[0]\n",
    "    pred = pred[:,0].cpu().detach().numpy()\n",
    "    print ((pred==0).sum())\n",
    "    print ((pred==1).sum())\n",
    "    print (f'验证集上, roc的值为{roc_auc_score(y_valid.cpu().detach().numpy(),pred)}')\n",
    "\n",
    "net_eval(net,X_valid_t,y_valid_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照方案一的方式，将数据集的正负样本均衡化\n",
    "\n",
    "防止正负样本比例失调，对模型造成影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_indices = (y_train==0)\n",
    "true_indices = (y_train==1)\n",
    "\n",
    "x_train_true = X_train[true_indices]\n",
    "y_train_true = y_train[true_indices]\n",
    "\n",
    "x_train_false = X_train[false_indices]\n",
    "y_train_false = y_train[false_indices]\n",
    "\n",
    "true_count = y_train_true.shape[0]\n",
    "x_train_false_list = []\n",
    "y_train_false_list = []\n",
    "for i in range(15):\n",
    "    if i < 14:\n",
    "        x_train_false_list.append(x_train_false[true_count*i:(i+1)*true_count])\n",
    "        y_train_false_list.append(y_train_false[true_count*i:(i+1)*true_count])\n",
    "    else:\n",
    "        x_train_false_list.append(x_train_false[true_count*i:])\n",
    "        y_train_false_list.append(y_train_false[true_count*i:])\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "for i in range(len(x_train_false_list)):\n",
    "    x_train_i = x_train_false_list[i]\n",
    "    y_train_i = y_train_false_list[i]\n",
    "    # 将正负类进行连接，并打乱\n",
    "    x_train_son = np.concatenate((x_train_i,x_train_true),axis=0)\n",
    "    y_train_son = np.concatenate((y_train_i,y_train_true),axis=0)\n",
    "    shuffler = np.random.permutation(len(x_train_son))\n",
    "    x_train_son = x_train_son[shuffler]\n",
    "    y_train_son = y_train_son[shuffler]\n",
    "    x_train_list.append(x_train_son)\n",
    "    y_train_list.append(y_train_son)\n",
    "\n",
    "del x_train_true,x_train_false,y_train_true,y_train_false,x_train_false_list,y_train_false_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即便使用这20000个样本来训练，样本虽然均衡了，但是预测效果不怎么样，其对应的auc的值也很低\n",
    "\n",
    "综合分析，放弃方案二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_valid_t = torch.Tensor(X_valid).to(device)\n",
    "y_valid_t = torch.Tensor(y_valid).long().to(device)\n",
    "X_test_t = torch.Tensor(test_data).to(device)\n",
    "net = Net(in_dim,hidden_dim_list,out_dim).to(device)\n",
    "\n",
    "# 将原始数据转化为tensor类型\n",
    "for i in range(len(x_train_list)):\n",
    "    if i == 0:\n",
    "        device = torch.device('cuda')\n",
    "        X_train_i = x_train_list[i]\n",
    "        y_train_i = y_train_list[i]\n",
    "\n",
    "        X_train_t = torch.Tensor(x_train_i).to(device)\n",
    "        y_train_t = torch.Tensor(y_train_i).long().to(device)\n",
    "        net_train(net,X_train_t,y_train_t,batch_size=2,epoch=1,verbose=0)\n",
    "        net_eval(net,X_valid_t,y_valid_t)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方案三：过采样，使正负样本均衡后，再预测算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 使用smote进行过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import model_train\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 过采样这么快，震惊我一整年\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_smo,y_smo = smote.fit_sample(X_train,y_train)\n",
    "X_val_smo,y_val_smo = smote.fit_sample(X_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:42:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "模型XGBoost:\n",
      "模型XGBoost, 耗时 1.895048983891805 minutes:\n",
      "使用clf.score计算, 测试集的准确度为0.9663319979856363\n",
      "测试集的ROC面积为0.9838324282423343\n",
      "测试集的混淆矩阵为:\n",
      "[[220281    136]\n",
      " [ 14706 205711]]\n",
      "测试集的分类报告为:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97    220417\n",
      "         1.0       1.00      0.93      0.97    220417\n",
      "\n",
      "    accuracy                           0.97    440834\n",
      "   macro avg       0.97      0.97      0.97    440834\n",
      "weighted avg       0.97      0.97      0.97    440834\n",
      "\n",
      "[18:44:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "模型XGBoost:\n",
      "模型XGBoost, 耗时 1.9691530187924704 minutes:\n",
      "使用clf.score计算, 测试集的准确度为0.9658297611757501\n",
      "测试集的ROC面积为0.9806989949792504\n",
      "测试集的混淆矩阵为:\n",
      "[[24476    19]\n",
      " [ 1655 22840]]\n",
      "测试集的分类报告为:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97     24495\n",
      "         1.0       1.00      0.93      0.96     24495\n",
      "\n",
      "    accuracy                           0.97     48990\n",
      "   macro avg       0.97      0.97      0.97     48990\n",
      "weighted avg       0.97      0.97      0.97     48990\n",
      "\n",
      "[18:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "模型XGBoost:\n",
      "模型XGBoost, 耗时 1.4940171122550965 minutes:\n",
      "使用clf.score计算, 测试集的准确度为0.938628435619274\n",
      "测试集的ROC面积为0.7040285372566035\n",
      "测试集的混淆矩阵为:\n",
      "[[24476    19]\n",
      " [ 1582    10]]\n",
      "测试集的分类报告为:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97     24495\n",
      "         1.0       0.34      0.01      0.01      1592\n",
      "\n",
      "    accuracy                           0.94     26087\n",
      "   macro avg       0.64      0.50      0.49     26087\n",
      "weighted avg       0.90      0.94      0.91     26087\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8,\n",
       "              enable_categorical=False, eta=0.3, gamma=0, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=8,\n",
       "              min_child_weight=300, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=-1, num_parallel_tree=1,\n",
       "              objective='binary:logistic', predictor='auto', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=42,\n",
       "              subsample=0.8, tree_method='exact', use_label_encoder=True, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_train('logistic',model=model_dict['logistic'],x_train=X_smo,y_train=y_smo,x_val=X_valid,y_val=y_valid)\n",
    "# model_train('bayes',model=model_dict['bayes'],x_train=X_smo,y_train=y_smo,x_val=X_valid,y_val=y_valid)\n",
    "# model_train('decision_tree',model=model_dict['decision_tree'],x_train=X_smo,y_train=y_smo,x_val=X_valid,y_val=y_valid)\n",
    "# model_train('GBDT',model=model_dict['GBDT'],x_train=X_train,y_train=y_train,x_val=X_valid,y_val=y_valid)\n",
    "model_train('XGBoost',model=model_dict['XGBoost'],x_train=X_smo,y_train=y_smo,x_val=X_smo,y_val=y_smo)\n",
    "model_train('XGBoost',model=model_dict['XGBoost'],x_train=X_smo,y_train=y_smo,x_val=X_val_smo,y_val=y_val_smo)\n",
    "model_train('XGBoost',model=model_dict['XGBoost'],x_train=X_smo,y_train=y_smo,x_val=X_valid,y_val=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 重复过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(435817,)\n",
      "220417 215400\n"
     ]
    }
   ],
   "source": [
    "# X_smo,y_smo = smote.fit_sample(X_train,y_train)\n",
    "\n",
    "X_train_true_rep = np.repeat(X_train[(y_train==1)],15,axis=0)\n",
    "x_train_false_rep = X_train[(y_train==0)]\n",
    "y_train_true_rep = np.ones(X_train_true_rep.shape[0])\n",
    "y_train_false_rep = np.zeros(x_train_false_rep.shape[0])\n",
    "\n",
    "\n",
    "x_train_rep = np.concatenate((X_train_true_rep,x_train_false_rep),axis=0)\n",
    "y_train_rep = np.concatenate((y_train_true_rep,y_train_false_rep),axis=0)\n",
    "\n",
    "indices = np.random.permutation(x_train_rep.shape[0])\n",
    "x_train_rep = x_train_rep[indices]\n",
    "y_train_rep = y_train_rep[indices]\n",
    "del X_train_true_rep,x_train_false_rep,y_train_true_rep,y_train_false_rep\n",
    "\n",
    "print (y_train_rep.shape)\n",
    "print ((y_train_rep==0).sum(),(y_train_rep==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:47:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "模型XGBoost:\n",
      "模型XGBoost, 耗时 0.9007354537645976 minutes:\n",
      "使用clf.score计算, 测试集的准确度为0.7479515484710325\n",
      "测试集的ROC面积为0.8281732020717936\n",
      "测试集的混淆矩阵为:\n",
      "[[162170  58247]\n",
      " [ 51600 163800]]\n",
      "测试集的分类报告为:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.74      0.75    220417\n",
      "         1.0       0.74      0.76      0.75    215400\n",
      "\n",
      "    accuracy                           0.75    435817\n",
      "   macro avg       0.75      0.75      0.75    435817\n",
      "weighted avg       0.75      0.75      0.75    435817\n",
      "\n",
      "[18:48:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "模型XGBoost:\n",
      "模型XGBoost, 耗时 0.8283499558766683 minutes:\n",
      "使用clf.score计算, 测试集的准确度为0.7085904856825238\n",
      "测试集的ROC面积为0.6991876482842873\n",
      "测试集的混淆矩阵为:\n",
      "[[17596  6899]\n",
      " [  703   889]]\n",
      "测试集的分类报告为:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.72      0.82     24495\n",
      "         1.0       0.11      0.56      0.19      1592\n",
      "\n",
      "    accuracy                           0.71     26087\n",
      "   macro avg       0.54      0.64      0.51     26087\n",
      "weighted avg       0.91      0.71      0.78     26087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model_train('XGBoost',model=model_dict['XGBoost'],x_train=x_train_rep,y_train=y_train_rep,x_val=x_train_rep,y_val=y_train_rep)\n",
    "model = model_train('XGBoost',model=model_dict['XGBoost'],x_train=x_train_rep,y_train=y_train_rep,x_val=X_valid,y_val=y_valid)\n",
    "# model_train('XGBoost',model=model_dict['XGBoost'],x_train=x_train_rep,y_train=y_train_rep,x_val=X_valid,y_val=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = './data_format1'\n",
    "submission = pd.read_csv(f'{paths}/test_format1.csv')\n",
    "\n",
    "prob = model(test_data)[:,1]\n",
    "submission['prob'] = prob\n",
    "# submission.dorp(['origin'], axis=1, inplace=True)\n",
    "submission.to_csv('./result/submission_oversample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方案三总结：\n",
    "\n",
    "1. 过采样在训练集能获得很高的准确度与auc面积，但是这是基于正负样本的属于1：1的比例\n",
    "\n",
    "2. 验证集上测试，表现效果异常的差。\n",
    "\n",
    "3. 该方案也不能有效提升auc指标的值。同样抛弃\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目总结\n",
    "\n",
    "1. 最终发现能用的也只有方案一，提交给天池平台最高能达到0.6871的auc值。\n",
    "\n",
    "2. 方案二、三体现的是思考过程，在此项目中就是关于不平衡数据处理的思考。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "579282893146967bc5d17141c601e8a8b8ad4a0a2a5a6fe1c87b1000077400d7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
